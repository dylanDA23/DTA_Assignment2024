{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7cb1cd-0006-4e69-a824-9e5607c422ca",
   "metadata": {},
   "source": [
    "# Data Analytics Assignment 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ad76f-30d7-4def-a9f3-0b25c92e6518",
   "metadata": {},
   "source": [
    "## Problem Statement: How different factors effect student performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c526d-39a9-4e37-a554-f6269ad1ac64",
   "metadata": {},
   "source": [
    "## LOADING AND CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a68c86-7f9a-441b-b0ab-23d69d04f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making all the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cebf85-8228-49f3-ac6f-08432fdb14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "stdData = pd.read_csv(\"StudentPerformanceFactors.csv\")\n",
    "stdData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec0219-046f-4fd1-b816-3baa52c01e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2204dad-ae16-4803-a878-dad939b4f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d62f16-d37e-4642-9041-211b134da644",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1a864-a480-427d-ae81-f6083df9ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5c6c4-8bbe-4cac-96a9-b6965311cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletion of irrelevant columns\n",
    "stdData = stdData.drop(columns=['Extracurricular_Activities', 'Previous_Scores', 'Motivation_Level', 'Tutoring_Sessions', 'Peer_Influence', 'Physical_Activity', 'Learning_Disabilities', 'Parental_Education_Level', 'Distance_from_Home', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac63140-6148-4bfe-8005-6dfff9215e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7c548-536c-4239-8c23-d6f5f4d2f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cf5bb-e1bb-403e-adf3-e7643a593e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies duplicate columns\n",
    "stdData.columns.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e0aff-f190-4f04-ab38-eb26b4bf353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks for missing (NaN) values in the stdData DataFrame and returns a count of how many missing values are in each column.\n",
    "missing_values = stdData.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5fd36-4b9b-487f-8b6d-06d4a43744b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the mode of column 'Teacher_Quality' to use it to replace null cells\n",
    "mode_value = stdData['Teacher_Quality'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d472aa-93fb-4945-bce7-d5f3a336cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff983b-f077-40d4-86e9-7719748d5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.fillna(mode_value, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93d3bb-2b77-411e-8473-2e034817f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are still empty cells\n",
    "missing_values = stdData.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c494d0d-658a-4962-8ce6-e1104da5d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for NaN or out-of-range values in Exam_Score\n",
    "invalid_exam_scores = stdData[(stdData['Exam_Score'].isnull()) | (stdData['Exam_Score'] < 0) | (stdData['Exam_Score'] > 100)]\n",
    "print(invalid_exam_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0244c5-b63d-4229-8cb8-a6c442ade00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes sure no mark exceeds 100 in the Exam_Score Table by replacing them with 100\n",
    "stdData['Exam_Score'] = stdData['Exam_Score'].clip(upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d3ca4-e72c-42cb-8cbd-7b43b8c9e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the code worked\n",
    "invalid_exam_scores = stdData[(stdData['Exam_Score'].isnull()) | (stdData['Exam_Score'] < 0) | (stdData['Exam_Score'] > 100)]\n",
    "print(invalid_exam_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b38db4-0ca8-4eaf-a264-523797220ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36cbc3-421b-4ec8-87e9-54ada5b7dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The exam score column has too many different values, which diminishes the capacity of the algorithm to make the right prediction.\n",
    "#We are making a new column that will group ranges of scores into categories.\n",
    "bins = [0, 50, 60, 70, 75, 101]  # Ranges for Fail, Pass, Credit, Merit, Distinction\n",
    "labels = ['Fail', 'Pass', 'Credit', 'Merit', 'Distinction']  # Corresponding performance categories\n",
    "\n",
    "# New column 'Performance_Category' being created based on the 'Exam_Score'\n",
    "stdData['Performance_Category'] = pd.cut(stdData['Exam_Score'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Displaying the first few rows to verify\n",
    "print(stdData[['Exam_Score', 'Performance_Category']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69e751-3e81-46c5-9843-98a2c4f26310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a copy of our table before deleting the Exam_Score column \n",
    "stdDataOriginal = stdData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09156197-dc32-4c23-82f2-c881bd7ea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdDataOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db0c5c-d902-4fcf-9978-9fe11d3811ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We imported this module to help us change string data to numeric data.\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f163cf-3ed4-4f19-94c0-7757027ff87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using the above module to pass the column name where our categorical data is. \n",
    "# It is worth mentioning that this approach wouldn't work well in columns with no Inherent order -- categories with no natural order like 'Red', 'Green', Yellow'\n",
    "stdData['Parental_Involvement'] = le.fit_transform(stdData['Parental_Involvement'])\n",
    "stdData['Access_to_Resources'] = le.fit_transform(stdData['Access_to_Resources'])\n",
    "stdData['Internet_Access'] = le.fit_transform(stdData['Internet_Access'])\n",
    "stdData['Family_Income'] = le.fit_transform(stdData['Family_Income'])\n",
    "stdData['Teacher_Quality'] = le.fit_transform(stdData['Teacher_Quality'])\n",
    "stdData['School_Type'] = le.fit_transform(stdData['School_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e3a48-130f-44fb-9721-198fae5dd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.head() #visualizing changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359b3ae-7b43-46aa-b8c2-205808ea0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Exam_Score\n",
    "stdData = stdData.drop('Exam_Score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d22b58-955d-4acf-a6b4-dc591849ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586c80-9bb0-4941-8044-9669488282c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the target column\n",
    "print(stdData['Performance_Category'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc711d1-b3ce-4426-91b1-d6ff864ab132",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYTICS: GRAPHS AND VISUALISATIONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086290b-e258-496b-8e45-9e4d6b056886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the graphs here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df97cf-b169-449c-a4ef-938be5d21cc8",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING: VALIDATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4a162-6644-407c-b7b2-7780373b0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We imported this module to help us change string data to numeric data.\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8c642-93cd-4eb7-a358-3b66440dd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using the above module to pass the column name where our categorical data is. \n",
    "# It is worth mentioning that this approach wouldn't work well in columns with no Inherent order -- categories with no natural order like 'Red', 'Green', Yellow'\n",
    "stdData['Parental_Involvement'] = le.fit_transform(stdData['Parental_Involvement'])\n",
    "stdData['Access_to_Resources'] = le.fit_transform(stdData['Access_to_Resources'])\n",
    "stdData['Internet_Access'] = le.fit_transform(stdData['Internet_Access'])\n",
    "stdData['Family_Income'] = le.fit_transform(stdData['Family_Income'])\n",
    "stdData['Teacher_Quality'] = le.fit_transform(stdData['Teacher_Quality'])\n",
    "stdData['School_Type'] = le.fit_transform(stdData['School_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7024d-eb43-4f3a-86de-6f61c7a0327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdData.head() #visualizing changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e7a7e-37f0-478b-b7b1-fc6a9b8533d6",
   "metadata": {},
   "source": [
    "## Machine :earning: Building our model\n",
    "### With our dataset ready we can now build our model and test various algorithms using the cross valiation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630b5bf-208a-41ba-9cb6-4c53e63ec317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing our data frame values to an array and dividing our data set into training data(x) and testing data(y)\n",
    "array = stdData.values\n",
    "X = array[:, 0:9]\n",
    "Y = array[:, 9]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efb25f-60bf-40ad-afe7-d8edabaa35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d461f80-ee43-4d97-90f5-f9d3bc8ef30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to scale our training data because I was getting some an error in the logistic regression algorithm\n",
    "# Error: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
    "#The solution was to scale the data set using the function below.\n",
    "# StandardScaler helps normalize the features to have a mean of 0 and a standard deviation of 1, which can improve convergence during training.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ae7d0-c6c7-4f20-a27a-2024c3767b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(max_iter=200))) \n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Evaluate each model\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    # Use the unscaled data for all models except Logistic Regression\n",
    "    if name == 'LR':\n",
    "        cv_results = model_selection.cross_val_score(model, X_train_scaled, Y_train, cv=kfold, scoring=scoring) ## Using our scaled train set here\n",
    "    else:\n",
    "        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ca52c-51f0-4197-9c27-91b2b86acf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for better visualization of scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(results, tick_labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed277c-f20a-47bc-9cdc-c9661a9f5e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = LogisticRegression(max_iter=200)\n",
    "lda.fit(X_train, Y_train)\n",
    "predictions = lda.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(\"\\t=============================================\\n\")\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(\"\\t=============================================\\n\")\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd12ad-8783-4abe-b144-592874aa0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stdData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e23c8-243e-445e-924e-35fe472a90ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of the resulting datasets\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4bb53-d812-4d9f-946e-87087598ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=seed)\n",
    "dt_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c8c79-1b77-4e8a-a795-30bf38f47596",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING: FEATURE OF IMPORTANCE\n",
    "Still in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e3e11-7bae-4f9a-ac7c-1a496e9d5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = dt_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importances = pd.DataFrame({'Feature': stdData.columns[0:9], 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances\n",
    "print(feature_importances)\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='lightgreen')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Student Performance Prediction using Decision Tree')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a68ab47-93f5-45d0-aa67-21de9c312d99",
   "metadata": {},
   "source": [
    "###EXPERIMENT\n",
    "\n",
    "from sklearn.inspection import permutation_importance  # Import permutation_importance\n",
    "\n",
    "# Create models\n",
    "models = [\n",
    "    ('LR', LogisticRegression(max_iter=200)),\n",
    "    ('LDA', LinearDiscriminantAnalysis()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('CART', DecisionTreeClassifier()),\n",
    "    ('NB', GaussianNB()),\n",
    "    ('SVM', SVC(kernel='linear'))  # Use linear kernel for feature importance\n",
    "]\n",
    "\n",
    "# Fit models and get feature importance\n",
    "feature_importances = {}\n",
    "for name, model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    if name in ['LR', 'LDA', 'SVM']:  # For linear models\n",
    "        importance = np.abs(model.coef_[0])  # For logistic regression and LDA\n",
    "    elif name == 'CART':  # Decision tree\n",
    "        importance = model.feature_importances_\n",
    "    elif name == 'KNN':  # KNN doesn't have a direct measure of feature importance\n",
    "        # Calculate permutation importance\n",
    "        result = permutation_importance(model, X_validation, Y_validation, n_repeats=30, random_state=7)\n",
    "        importance = result.importances_mean\n",
    "    elif name == 'NB':  # Naive Bayes does not provide feature importance directly\n",
    "        importance = None  # Could implement alternative methods if needed\n",
    "    \n",
    "    feature_importances[name] = importance\n",
    "\n",
    "# Display feature importances\n",
    "for name, importance in feature_importances.items():\n",
    "    if importance is not None:\n",
    "        print(f\"{name} Feature Importances: {importance}\")\n",
    "    else:\n",
    "        print(f\"{name} does not provide direct feature importances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5108f4-0e6e-449b-8d00-f9ed98c02793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
